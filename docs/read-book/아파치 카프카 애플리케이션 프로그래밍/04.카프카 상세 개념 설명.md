# 토픽과 파티션
## 적정 파티션 개수
- 토픽의 파티션 개수는 카프카의 성능과 관련 있음
- 토픽 생성 시 파티션 개수 고려사항
  - 데이터 처리량
  - 메시지 키 사용 여부
  - 브로커, 컨슈머 영향도
- 파티션은 카프카의 병렬처리의 핵심
- 데이터 처리 속도 올리는 법
  - 컨슈머 자체의 처리량 늘림
    - 컨슈머 서버의 스케일 업을 하거나 GC 튜닝 등
    - 일정 수준 이상의 처리량을 올리기 힘듬
  - 컨슈머를 추가해서 병렬 처리량 늘림
    - 파티션 개수를 늘리고 파티션 개수만큼 컨슈머를 추가하는 방법은 데이터 처리량을 늘리는 가장 확실한 방법
- 파티션 개수 공식 : 프로듀서 전송 데이터량 < 컨슈머 데이터 처리량 * 파티션 개수
- 파티션 개수를 무작정 늘리면 브로커의 부담이 발생할 수 있음
- 메시지 키 사용 여부는 데이터 처리 순서와 밀접하게 연관되어 있음
  - 메시지 키를 사용하고 컨슈머에서 처리 순서가 보장되어야 한다면 최대한 파티션의 변화가 발생하지 않게 운영해야 함

## 토픽 처리 정책(cleanup.policy)
삭제와 압축의 2가지 삭제 정책 제공

### 토픽 삭제 정책(delete policy)
- 일반적으로 설정하는 정책
- `cleanup.policy` 를 `delete` 로 설정
  - 명시적으로 토픽의 데이터를 삭제함
  - 세그먼트 단위로 삭제
  > 세그먼트는 토픽의 데이터를 저장하는 명시적인 파일 시스템 단위
- 삭제 정책이 실행되는 기준
  - `retention.ms` : 토픽의 데이터를 유지하는 기간을 밀리초로 설정
    - 일정 주기마다 세그먼트 파일의 마지막 수정 시간과 `retention.ms`를 비교하여 세그먼트 파일의 마지막 수정 시간이 `retention.ms`를 넘어가면 세그먼트는 삭제
  - `retention.byte` : 토픽의 최대 데이터 크기 제어
    - `retention.byte`를 넘어간 세그먼트 파일 삭제
- 삭제된 데이터는 복구 불가능

### 토픽 압축 정책(compact policy)
- 메시지 키별로 해당 메시지 키의 레코드 중 오래된 데이터를 삭제하는 정책
- 카프카 스트림즈의 `KTable`과 같이 메시지 키를 기반으로 데이터를 처리할 경우 유용
- `min.cleanable.dirty.ratio` : 액티브 세그먼트를 제외한 세그먼트에 남아 있는 데이터의 tail 영역의 레코드 개수와 head 영역의 레코드 개수의 비율
  - tail 영역의 레코드들을 clean log 라고 부르고 압축이 완료됐기 때문에 중복된 메시지 키가 없음
  - head 영역의 레코드들은 dirty log 라고 부르고 압축이 되기 전 레코드들이 있으므로 중복된 메시지 키를 가진 레코드가 있음
  - 더티 레코드 개수 / (클린 레코드 개수 + 더티 레코드 개수) => 액티브 세그먼트의 레코드는 계산에서 제외

## ISR(In-Sync-Replicas)
- 리더 파티션과 팔로워 파티션이 모두 싱크가 된 상태
- `replica.lag.time.max.ms` : 리더와 팔로워의 복제 시간차 때문에 오프셋의 차이가 발생하는 데, 이런 차이를 모니터링 하기 위해 이 설정 값만큼의 주기를 가지고 팔로워 파티션이 데이터를 복제하는 지 확인
  - 팔로워 파티션이 `replica.lag.time.max.ms` 값보다 긴 시간 동안 데이터를 가져가지 않으면 해당 팔로워 파티션에 문제가 생긴 것으로 판단하고 ISR 그룹에서 제외
- `ISR`로 묶이지 못한 팔로워 파티션은 리더로 선출될 자격이 없음
  - `unclean.leader.election.enable` : `ISR` 관련 리더 선출 자격 옵션
    - `false` : `ISR`이 아닌 팔로워 파티션을 리더 파티션으로 선출하지 않음
      - `ISR`에 포함된 파티션이 없으면 서비스가 중단될 수 있음
      - 리더 파티션이 존재하는 브로커가 다시 시작될때까지 기다림
      - 메시지가 유실되지 않음
    - `true` : `ISR`이 아닌 팔로워 파티션도 리더로 선출될 수 있음
      - 서비스가 중단될 가능성이 적음 (모든 브로커가 다운되는 경우는 잘 없음)
      - 메시지가 유실될 가능성이 있음
    
# 카프카 프로듀서
## acks 옵션
### acks=0
- 프로듀서가 리더 파티션으로 데이터를 전송했을 때 리더 파티션으로 데이터가 저장되었는지 확인하지 않음
- 확인하지 않기 때문에 전송 실패 여부를 알 수 없고, 그렇기 때문에 재시도를 하지 않아 `retries` 옵션이 무의미
- 데이터가 일부 유실되더라도 전송 속도가 중요할 때 사용

### acks=1
- 리더 파티션에만 정상적으로 적재되었는 지 확인
  - 팔로워 파티션에는 아직 데이터가 동기화 되어있지 않을 수 있기 때문에, 팔로워 파티션에 데이터가 복제되기 전에 리더 파티션에 장애가 발생하면 동기화 되지 않은 데이터가 유실될 수 있음
- acks=0 보다는 느리고, all 보다는 빠름

### acks=all
- 리더 파티션과 팔로워 파티션에 모두 정상적으로 적재되었는 지 확인
- 속도가 가장 느림
- `min.insync.replicas` : `ISR` 그룹에 속한 리더+팔로워 파티션에 최소 복제수
  - 해당 옵션의 수만큼 충족되면 정상 적재로 인정
  - 옵션 값에 따라 데이터 안정성이 달라짐 => 수가 높을 수록 안정성이 높아지고, 속도는 떨어짐
  - 리더 파티션의 수까지 포함된다는 것을 고려해야 함 => 따라서 2 이상으로 설정해야 의미 있음
  - 실제로 카프카 클러스터를 운영하며 브로커가 동시에 2개가 중단되는 일은 극히 드물기 때문에 2로 설정하여 적재가 완료되었다면 데이터는 유실되지 않는 다고 볼 수 있음
  - **브로커 개수와 동일한 숫자로 설정하면 안됨**
    - 카프카 클러스터 버전 업그레이드와 같은 상황에서 롤링 다운 타임이 생기면 데이터 추가가 불가능 해짐
  
  ## 멱등성 프로듀서
  [참고](https://github.com/pch8388/study-tech/blob/main/docs/read-book/%EC%95%84%ED%8C%8C%EC%B9%98%20%EC%B9%B4%ED%94%84%EC%B9%B4%20%EC%95%A0%ED%94%8C%EB%A6%AC%EC%BC%80%EC%9D%B4%EC%85%98%20%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D/00.%ED%94%84%EB%A1%9C%EB%93%80%EC%84%9C%EC%A0%84%EC%86%A1%EB%B0%A9%EC%8B%9D.md)

  
  

# 생각해볼 것
- 메시지 키는 데이터 처리 순서를 결정짓기 위해 사용한다는 뉘앙스로 적혀있는 데 메시지 키를 쓰지 않는 것이 성능상 더 유리한가?
  - 핫스팟 발생 확률을 훨씬 낮추는 등의 효과는 있을 것 같다
- `replica.lag.time.max.ms` 값을 체크하는 기준? 해당 파티션에 데이터가 지속적으로 들어오는 상황이라면 해당 값을 어떻게 체크하나?
  - 만약 복제 자체를 실행할 때마다 해당 값이 리셋 되는 형태라면 특정 이유로 복제가 밀려서 오프셋 차이가 난다면?
  - 리더 오프셋과 특정 시간안에 같게 맞춰져야 한다면 파티션에 데이터가 계속 들어오면 오프셋 동일성 체크 시점이 계속 갱신될텐데 그것을 체크하는 부하는?
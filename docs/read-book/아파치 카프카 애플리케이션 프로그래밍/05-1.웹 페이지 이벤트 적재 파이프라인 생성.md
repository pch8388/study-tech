# 개요
- 웹 페이지의 이벤트를 수집하는 데이터 파이프라인 구축
- 카프카는 갑자기 많은 이벤트가 발생하여도 모두 카프카의 토픽에 쌓이고 컨슈머가 처리할 수 있는 만큼만 가져가서 처리하기 때문에 오류 발생 가능성이 낮다

# 요구사항
이름을 입력하고 자신이 좋아하는 색상을 고르는 버튼을 누르면, 해당 이벤트와 유저 에이전트를 카프카 토픽으로 전달하고 최종적으로 하둡과 엘라스틱서치에 적재되는 것을 목표로 한다.

# 기능 구현
## 로컬 하둡, 엘라스틱서치, 키바나 설치
```bash
# 하둡
$ brew install hadoop

# 엘라스틱서치
$ brew tap elastic/tap
$ brew install elastic/tap/elasticsearch-full

# 키바나
$ brew install elastic/tap/kibana-full
```
[Mac 엘라스틱서치 설치 참고](https://logz.io/blog/brew-install-elasticsearch-mac/)

### 하둡 설정
```bash
$ cd /opt/homebrew/Cellar/hadoop/3.3.3/libexec/etc/hadoop/
$ vi core-site.xml
```

```xml
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://localhost:9000</value>
  </property>
</configuration>
```

### 토픽 생성
```bash
$ bin/kafka-topics.sh --create --bootstrap-server my-kafka:9092 --replication-factor 1 --partitions 3 --topic select-color
```

## 하둡 적재 컨슈머 애플리케이션
- 하나의 프로세스에서 여러개의 쓰레드를 사용하여 적재
- 파티션 개수와 쓰레드 개수를 동일하게 하면 최적의 성능을 낼 수 있음
- 토픽의 데이터가 급속도로 늘어나서 파티션을 늘리더라도 컨슈머의 스레드 개수를 효과적으로 늘릴 수 있도록 쓰레드 개수를 변수로 선언하여 사용
- 데이터를 저장하는 방식 2가지
  - append : 파일에 내용을 계속 추가, 파일에 문제가 생기면 append 불가
  - flush : 버퍼 메모리에 담아두었다가 write, 파일에 문제가 생겨도 flush 가능하지만, 파일개수가 늘어나고, 버퍼로 사용할 메모리 필요
    - AWS S3 는 append 지원하지 않음
- 멀티쓰레드 환경이기 때문에 동일 데이터의 동시 접근을 유의해야 함
  - 파일에 동시 접근 시 데드락에 빠질 수 있음
  - 간단한 해결책은 파티션 번호에 따라 파일을 따로 저장

## 엘라스틱서치 싱크 커넥터 개발
- 커넥터는 그 자체로 애플리케이션 단위로 실행되지 않고 플러그인으로 추가하여 실행해야 함

# 생각해볼점
- 1프로세스 N쓰레드 컨슈머는 어느 정도까지 쓰레드 개수를 늘려서 하나의 프로세스로 쓰는 것이 효율적일까?